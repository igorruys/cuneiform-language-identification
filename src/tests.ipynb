{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ../data/datasets/cuneiform/train\\cache-a615edd2ff1aaf10.arrow\n",
      "Loading cached processed dataset at ../data/datasets/cuneiform/val\\cache-177b1de2c1bef4c2.arrow\n",
      "Loading cached processed dataset at ../data/datasets/cuneiform/test\\cache-44006bcc82ef534d.arrow\n",
      "Loading cached processed dataset at ../data/datasets/cuneiform/train\\cache-d80c26604b24b48d.arrow\n",
      "Loading cached processed dataset at ../data/datasets/cuneiform/val\\cache-7309d4f149fb1b31.arrow\n",
      "Loading cached processed dataset at ../data/datasets/cuneiform/test\\cache-247eff21687e41df.arrow\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "from evaluate import load\n",
    "from datasets import load_from_disk\n",
    "from transformers import (AutoTokenizer, AlbertForPreTraining,\n",
    "                          DefaultDataCollator, AutoConfig,\n",
    "                          TrainingArguments, Trainer,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          AlbertForMaskedLM)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    '''\n",
    "    This function should be given to map for dataset tokenization.\n",
    "    ------------------------------------------------------------------\n",
    "    Arg:\n",
    "        example: Dictionary-like object containing examples of the \n",
    "        dataset (Ex: {'text':..., 'label':...}). If the option batch \n",
    "        in map is set to False, each key of the dictionary is attached\n",
    "        to one single example. Otherwise, each keys is attached to\n",
    "        list of examples.\n",
    "    Returns:\n",
    "        output: A dictionary with keys input_ids and attention_mask.\n",
    "        Its values may be a single list (for map's batch option set to\n",
    "        False) or a list of lists (batch set to True).\n",
    "    '''\n",
    "    output = tokenizer(example[\"text\"], truncation=True, max_length=max_len)\n",
    "    del output['token_type_ids']\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_chunks(examples):\n",
    "    '''\n",
    "    Creates a dataset of chunks of data.\n",
    "    ------------------------------------------------------------------\n",
    "    Args:\n",
    "        examples (dict): Dictionary-like object, whose values are \n",
    "        lists of lists.\n",
    "    Returns:\n",
    "        dict_chunks (dict): A dictionary with the same keys (plus a \n",
    "        copy of the key \"input_ids\"), but whose values are lists of \n",
    "        chunks (lists) of same size.\n",
    "    '''\n",
    "    chunk_size = max_len\n",
    "\n",
    "    #For each key (inputs_ids, attention_mask, ...), we concatenate\n",
    "    #all its lists\n",
    "    concatenated_examples = {k: list(itertools.chain(*examples[k])) \n",
    "                            for k in examples.keys()}\n",
    "    \n",
    "    #We calculate the maximum number of chunks that can be formed from\n",
    "    #concatenated_examples.\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    max_num_chunks = total_length // chunk_size\n",
    "\n",
    "    #We create chunks from concatenated_examples. If total_length is\n",
    "    #not a multiple of chunk_size, then the remainder will be\n",
    "    #discarded.\n",
    "    dict_chunks = {k:[concatenated_examples[k][chunk_size*i:(i+1)*chunk_size]\n",
    "                   for i in range(max_num_chunks)]\n",
    "                   for k in concatenated_examples.keys()}\n",
    "\n",
    "    #We create a copy of input_ids that will be used as a reference\n",
    "    #for masked language modeling during training.\n",
    "    dict_chunks[\"labels\"] = dict_chunks[\"input_ids\"].copy()\n",
    "    return dict_chunks\n",
    "\n",
    "\n",
    "def compute_perplexity(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return perplexity.compute(predictions=predictions, references=labels)\n",
    "\n",
    "#Loading dataset\n",
    "dataset = load_from_disk('../data/datasets/cuneiform/')\n",
    "\n",
    "#Loading tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('../tokenizers/tokenizer/')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "#Since only 0,01% of all data has more than 64 cuneiform characters\n",
    "#(or 2*64 -1, if we count the white spaces), we set max_len to this\n",
    "#value\n",
    "max_len = 2*64\n",
    "\n",
    "#Tokenized dataset that has a \"input_ids\" key (containg a list of\n",
    "#tokens ids) and \"attention_mask\" key (indicating which characters \n",
    "#should be attended).\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True,\n",
    "                                remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "#Dataset with examples organized in chunks of size max_len\n",
    "chuked_dataset = tokenized_dataset.map(create_chunks, batched=True)\n",
    "\n",
    "#Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True,\n",
    "                                                mlm_probability=0.15)\n",
    "\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "config = AutoConfig.from_pretrained('albert-xlarge-v2')\n",
    "model = AlbertForMaskedLM(config)\n",
    "\n",
    "dia = datetime.today().strftime(\"%Y-%M-%d\")\n",
    "hora = datetime.now().strftime(\"%H-%M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "c:\\Users\\igorr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 82017\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 54821040\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33migorruys\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\igorr\\OneDrive\\Área de Trabalho\\Projeto Cuneiform\\src\\wandb\\run-20230130_003043-2d6ntga4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/igorruys/huggingface/runs/2d6ntga4\" target=\"_blank\">../checkpoints/pretraining/meu_output_dia_2023-30-30_hora_00-30</a></strong> to <a href=\"https://wandb.ai/igorruys/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/igorruys/huggingface\" target=\"_blank\">https://wandb.ai/igorruys/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/igorruys/huggingface/runs/2d6ntga4\" target=\"_blank\">https://wandb.ai/igorruys/huggingface/runs/2d6ntga4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0208d82ae18484396eeca6e88c4fbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.2812, 'learning_rate': 9.9e-06, 'epoch': 0.0}\n",
      "{'loss': 10.1616, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9113\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 9.7e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be11d83e6ff4638b977a86ca715a2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.32 GiB (GPU 0; 4.00 GiB total capacity; 2.13 GiB already allocated; 442.35 MiB free; 2.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\igorr\\OneDrive\\Área de Trabalho\\Projeto Cuneiform\\src\\tests.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/igorr/OneDrive/%C3%81rea%20de%20Trabalho/Projeto%20Cuneiform/src/tests.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/igorr/OneDrive/%C3%81rea%20de%20Trabalho/Projeto%20Cuneiform/src/tests.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../checkpoints/pretraining/meu_output_dia_\u001b[39m\u001b[39m{\u001b[39;00mdia\u001b[39m}\u001b[39;00m\u001b[39m_hora_\u001b[39m\u001b[39m{\u001b[39;00mhora\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/igorr/OneDrive/%C3%81rea%20de%20Trabalho/Projeto%20Cuneiform/src/tests.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     overwrite_output_dir\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/igorr/OneDrive/%C3%81rea%20de%20Trabalho/Projeto%20Cuneiform/src/tests.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     report_to\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwandb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/igorr/OneDrive/%C3%81rea%20de%20Trabalho/Projeto%20Cuneiform/src/tests.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/igorr/OneDrive/%C3%81rea%20de%20Trabalho/Projeto%20Cuneiform/src/tests.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/igorr/OneDrive/%C3%81rea%20de%20Trabalho/Projeto%20Cuneiform/src/tests.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/igorr/OneDrive/%C3%81rea%20de%20Trabalho/Projeto%20Cuneiform/src/tests.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/igorr/OneDrive/%C3%81rea%20de%20Trabalho/Projeto%20Cuneiform/src/tests.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_perplexity\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/igorr/OneDrive/%C3%81rea%20de%20Trabalho/Projeto%20Cuneiform/src/tests.ipynb#W1sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/igorr/OneDrive/%C3%81rea%20de%20Trabalho/Projeto%20Cuneiform/src/tests.ipynb#W1sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\igorr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1527\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1524\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1526\u001b[0m )\n\u001b[1;32m-> 1527\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1528\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1529\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1530\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1531\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1532\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\igorr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1852\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1849\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[0;32m   1850\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m-> 1852\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   1853\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1854\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\igorr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2115\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2109\u001b[0m             metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(\n\u001b[0;32m   2110\u001b[0m                 eval_dataset\u001b[39m=\u001b[39meval_dataset,\n\u001b[0;32m   2111\u001b[0m                 ignore_keys\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2112\u001b[0m                 metric_key_prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meval_\u001b[39m\u001b[39m{\u001b[39;00meval_dataset_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2113\u001b[0m             )\n\u001b[0;32m   2114\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2115\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[0;32m   2116\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n",
      "File \u001b[1;32mc:\\Users\\igorr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2811\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2808\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2810\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2811\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2812\u001b[0m     eval_dataloader,\n\u001b[0;32m   2813\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2814\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   2815\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   2816\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2817\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   2818\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   2819\u001b[0m )\n\u001b[0;32m   2821\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   2822\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   2823\u001b[0m     speed_metrics(\n\u001b[0;32m   2824\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2828\u001b[0m     )\n\u001b[0;32m   2829\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\igorr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:3016\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3014\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_logits_for_metrics \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3015\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n\u001b[1;32m-> 3016\u001b[0m     preds_host \u001b[39m=\u001b[39m logits \u001b[39mif\u001b[39;00m preds_host \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m nested_concat(preds_host, logits, padding_index\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m   3017\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_prediction_step(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   3019\u001b[0m \u001b[39m# Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\igorr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer_pt_utils.py:115\u001b[0m, in \u001b[0;36mnested_concat\u001b[1;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[39m=\u001b[39mpadding_index) \u001b[39mfor\u001b[39;00m t, n \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tensors, new_tensors))\n\u001b[0;32m    114\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39;49mpadding_index)\n\u001b[0;32m    116\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, Mapping):\n\u001b[0;32m    117\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(\n\u001b[0;32m    118\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[39m=\u001b[39mpadding_index) \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensors\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m    119\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\igorr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer_pt_utils.py:80\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[1;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[0;32m     77\u001b[0m new_shape \u001b[39m=\u001b[39m (tensor1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m tensor2\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mmax\u001b[39m(tensor1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], tensor2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m tensor1\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:]\n\u001b[0;32m     79\u001b[0m \u001b[39m# Now let's fill the result tensor\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m result \u001b[39m=\u001b[39m tensor1\u001b[39m.\u001b[39;49mnew_full(new_shape, padding_index)\n\u001b[0;32m     81\u001b[0m result[: tensor1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], : tensor1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m tensor1\n\u001b[0;32m     82\u001b[0m result[tensor1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] :, : tensor2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m tensor2\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.32 GiB (GPU 0; 4.00 GiB total capacity; 2.13 GiB already allocated; 442.35 MiB free; 2.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f'../checkpoints/pretraining/meu_output_dia_{dia}_hora_{hora}',\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy='steps',\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=1e-5,\n",
    "    #warmup_steps=100,\n",
    "    #logging_dir='log_tf',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=1,\n",
    "    eval_steps=3,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    compute_metrics=compute_perplexity\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(dataset['train']['text'][0], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  2,   5, 220,   5,   9,   5,  20,   3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 220, 5, 9, 5, 20, 3]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "output = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0008, -0.1967, -0.1836,  ...,  0.2964,  0.3753, -0.0686],\n",
       "         [-0.0008, -0.1967, -0.1836,  ...,  0.2964,  0.3753, -0.0685],\n",
       "         [-0.0008, -0.1967, -0.1836,  ...,  0.2964,  0.3752, -0.0685],\n",
       "         ...,\n",
       "         [-0.0008, -0.1966, -0.1836,  ...,  0.2963,  0.3753, -0.0686],\n",
       "         [-0.0008, -0.1967, -0.1836,  ...,  0.2964,  0.3752, -0.0685],\n",
       "         [-0.0008, -0.1968, -0.1836,  ...,  0.2964,  0.3753, -0.0685]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a80fcc3eed05bde34abc9291b0b7b239e074f72d5d5dd89f2751baaf0f2f807"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
