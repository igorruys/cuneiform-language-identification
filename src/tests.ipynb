{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from transformers import (AutoTokenizer, AlbertForPreTraining, DefaultDataCollator, AutoConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling,\n",
    "AlbertForMaskedLM, DataCollatorWithPadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ../data/datasets/cuneiform/train\\cache-2630f9322d2eae4d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40de40d0cd264a348f5830ae89a0bc87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ../data/datasets/cuneiform/test\\cache-ee5c050a6ad78515.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    '''\n",
    "    Function to be used during the tokenization of the dataset.\n",
    "    -------------------------------------------------------------\n",
    "    Args:\n",
    "        example: dictionary-like object that represents one example of the dataset,\n",
    "        that is, a single pair \"text\",\"label\" (Ex: {'text':..., 'label':...}).\n",
    "    Returns:\n",
    "        A dictionary with a single key input_ids, whose value is a list of token ids\n",
    "        corresponding to the characters, given example's 'text' key.\n",
    "    '''\n",
    "\n",
    "    #output is a dictionary with keys \"input_ids\", \"token_type_ids\" and\n",
    "    #\"attention mask\", whose values are lists of integers\n",
    "    output = tokenizer(example[\"text\"], truncation=True, max_length=max_len)\n",
    "    input_batch = []\n",
    "    for token_id in output['input_ids']:\n",
    "        input_batch.append(token_id)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "#Loading dataset\n",
    "dataset = load_from_disk('../data/datasets/cuneiform/')\n",
    "\n",
    "#Loading tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('../tokenizers/tokenizer/')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "#Since only 0,01% of all data has more than 64 for cuneiform characters (or 2*64 - 1, if we count the white spaces),\n",
    "#we set max_len to this value\n",
    "max_len = 2*64\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer(dataset['train']['text'][0],dataset['train']['text'][1])#,dataset['train']['text'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 𒂔 𒈾 𒆠[SEP] 𒊭 𒈗 𒁁 𒉌 𒋫 𒇽 𒄷 𒌒 𒋾[SEP]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(x['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 5, 220, 5, 9, 5, 20, 3],\n",
       " [2, 5, 40, 5, 46, 5, 57, 5, 15, 5, 26, 5, 27, 5, 70, 5, 111, 5, 30, 3],\n",
       " [2, 5, 156, 5, 24, 5, 12, 5, 62, 5, 44, 5, 143, 5, 98, 5, 12, 3],\n",
       " [2, 5, 31, 5, 319, 5, 85, 5, 142, 3]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train']['input_ids'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {'input_ids':tokenized_dataset['train']['input_ids'][:4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 5, 220, 5, 9, 5, 20, 3],\n",
       "  [2, 5, 40, 5, 46, 5, 57, 5, 15, 5, 26, 5, 27, 5, 70, 5, 111, 5, 30, 3],\n",
       "  [2, 5, 156, 5, 24, 5, 12, 5, 62, 5, 44, 5, 143, 5, 98, 5, 12, 3],\n",
       "  [2, 5, 31, 5, 319, 5, 85, 5, 142, 3]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([4, 20]), 'attention_mask': torch.Size([4, 20])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data_collator(samples)\n",
    "{k:np.shape(v) for k,v in y.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplo = tokenized_dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_examples = {k: sum(exemplo[k],[]) for k in exemplo.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 5, 220, 5, 9, 5, 20, 3],\n",
       " [2, 5, 40, 5, 46, 5, 57, 5, 15, 5, 26, 5, 27, 5, 70, 5, 111, 5, 30, 3],\n",
       " [2, 5, 156, 5, 24, 5, 12, 5, 62, 5, 44, 5, 143, 5, 98, 5, 12, 3]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemplo['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "x = list(itertools.chain(*tokenized_dataset['train']['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createChunks(examples):\n",
    "    '''\n",
    "    Creates chunks of data.\n",
    "    -------\n",
    "    Args:\n",
    "        examples (dict): Dictionary-like object, whose values are lists of lists.\n",
    "    Returns:\n",
    "        dict_chunks (dict): A dictionary with the same keys (plus a copy of the key \"input_ids\"),\n",
    "        but whose values are lists of chunks (lists) of same size.\n",
    "    '''\n",
    "    chunk_size = max_len\n",
    "\n",
    "    #For each key (inputs_ids, attention_mask, ...), we concatenate all its lists\n",
    "    concatenated_examples = {k: list(itertools.chain(*examples[k])) for k in examples.keys()}\n",
    "    \n",
    "    #We calculate the maximum number of chunks that can be formed from concatenated_examples.\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    max_num_chunks = total_length // chunk_size\n",
    "\n",
    "    #We create chunks from concatenated_examples.\n",
    "    #If total_length is not a multiple of chunk_size, then the remainder will be discarded.\n",
    "    dict_chunks = { k:[concatenated_examples[k][chunk_size*i:(i+1)*chunk_size] for i in range(max_num_chunks)] for k in concatenated_examples.keys()}\n",
    "\n",
    "    #We create a copy of input_ids that will be used as a reference during the training.\n",
    "    dict_chunks[\"labels\"] = dict_chunks[\"input_ids\"].copy()\n",
    "    return dict_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a80fcc3eed05bde34abc9291b0b7b239e074f72d5d5dd89f2751baaf0f2f807"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
